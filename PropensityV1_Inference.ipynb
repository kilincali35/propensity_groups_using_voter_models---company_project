{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KuC-UfMDhCOW"
      },
      "outputs": [],
      "source": [
        "cd ~/Ali/FBB/Propensity"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "n_cpu= os.cpu_count() print(n_cpu)\n"
      ],
      "metadata": {
        "id": "KPAfu7GYhHNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pymysql\n",
        "from sqlalchemy import create engine\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import feature_engine\n",
        "import psycopg2\n",
        "import warnings\n",
        "import mysql.connector\n",
        "import inspect\n",
        "import matplotlib.pyplot as plt \n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_columns', 40) \n",
        "pd.set_option('display.width', 120) \n",
        "import sqlalchemy as sa\n",
        "from sqlalchemy.engine \n",
        "import url as sa_url \n",
        "from datetime import datetime \n",
        "import timeit\n",
        "start_time = timeit.default_timer()"
      ],
      "metadata": {
        "id": "id_k9P1JhHQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ai = {}\n",
        "ai['user'] = 'user'\n",
        "ai['password'] = 'pwd'\n",
        "ai['name'] = 'AI'\n",
        "ai['host'] = '10.10.10.10'\n",
        "ai['port'] = 3333\n",
        "\n",
        "prod = {}\n",
        "prod['user'] = 'user'\n",
        "prod['password'] = 'pwd'\n",
        "prod['name'] = 'Prod'\n",
        "prod['host'] = '10.10.10.10'\n",
        "prod['port'] = 3333\n",
        "\n",
        "arch = {}\n",
        "arch['user'] = 'user'\n",
        "arch['password'] = 'pwd'\n",
        "arch['name'] = 'Archival'\n",
        "arch['host'] = '10.10.10.10'\n",
        "arch['port'] = 3333"
      ],
      "metadata": {
        "id": "0YgK-fhAuJ-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_gp_engine():\n",
        "  username = 'user',\n",
        "  password = 'pwd',\n",
        "  drivername = 'postgresql+psycopg2',\n",
        "  host = '1.2.3.4',\n",
        "  port = '1111',\n",
        "  database = 'gpafiniti'\n",
        "\n",
        "  url = sa_url.URL(drivername = drivername, host=host, port=port, database=database, username=username, password=password)\n",
        "\n",
        "  return sa.create_engine(url)\n",
        "\n",
        "def create_engine_arch_server(database):\n",
        "\n",
        "  conn = psycopg2.connect(\n",
        "      host = '1111',\n",
        "      database = database, \n",
        "      user = 'user',\n",
        "      password = 'pwd',\n",
        "      port = 1111)\n",
        "  return conn\n",
        "\n",
        "def run_query(cred,db,query):\n",
        "  try:\n",
        "    mydb=mysql.connector.connect(\n",
        "        host = cred['host'],\n",
        "        port = cred['port'],\n",
        "        user = cred['user'],\n",
        "        password = cred['password'],\n",
        "        database = db)\n",
        "    \n",
        "    df = pd.read_sql(quey,mydb)\n",
        "    mydb.close()\n",
        "    return df\n",
        "  except Exception as e:\n",
        "    print(str(e))\n",
        "\n",
        "def update_query(cred, db, query): \n",
        "  try:\n",
        "    mydb = mysql.connector.connect(\n",
        "      host=cred['host'],\n",
        "      port=cred['port'],\n",
        "      user=cred['user'],\n",
        "      password=cred['password'],\n",
        "      database=db)\n",
        "    \n",
        "    mycursor = mydb.cursor()\n",
        "    mycursor.execute(query)\n",
        "    mydb.commit()\n",
        "    mydb.close()\n",
        "    return print(mycursor.rowcount, \"record(s) affected\")\n",
        "  except Exception as e:\n",
        "    print(str(e))"
      ],
      "metadata": {
        "id": "d17H8WjJhHSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "script_pars ={\n",
        "    'explanation': 'Eliminated btn and some pivot variables, btns had high NA percentage',\n",
        "    'chunksize': 50000,\n",
        "    'skipdbread': 0, #If you already read DB succesfully and wrote into model_folder as csv batches, you can skip this part and write as csv part by making it 1 \n",
        "    'skippropwrite': 0, #If you already wrote propensity outputs into model folder as csv file batches, you can skip this part too\n",
        "                        #0: write everything, owerwrite can occur\n",
        "                        #1: If your kernel died during run, and you are sure input set didnt change, you can set it 1 to prevent overwrite, it wont save if name already there \n",
        "                        #2: skip everything, do not write any result, go on with what is already there.\n",
        "    'rmallpropcsv': 0, #if 1, it removes every result csv in directory. Be careful while using it\n",
        "    'nb_batches': 20, #If you already wrote data as csv batches, and you are skipping read DB part, you need to define nb of csv batches here, how many csv files do you have in directory \n",
        "    'sme_limit': ';', #Use it if you want to limit nrows for sme query. Otherwise just '' \n",
        "    'database': 'afiniti_aida_fbb', #sme database\n",
        "    'table_to_use': 'afiniti_aida_fbb.lookups joined', #name of the sme\n",
        "    'run_type': 'prod', #adds a suffix to models folder, you can use 'prod' or 'test'\n",
        "    'top_n_models': 3, #please use the number you used when you were training the model. I will flag used models in training pipe, but for now it should be manual\n",
        "    'preferred_folder_name': '2304061913-prod' #If you have a folder name containing pickle files you want to make predictions write it. \" is suggested, to get latest created one on run type"
      ],
      "metadata": {
        "id": "KXqQoYNVhHVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_dir = os.getcwd() + \"/model_results/\" + script_pars['preferred_folder_name'] + \"/\" \n",
        "\n",
        "columns_dict = {}\n",
        "with open(model_dir+\"/columns.txt\", 'r') as f:\n",
        "  columns_dict = f.read()"
      ],
      "metadata": {
        "id": "9tXZy_sqhHXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "dictt = ast.literal_eval(columns_dict) \n",
        "ordered cols= dictt['ordered_cols']\n",
        "cat_cols = dictt['cat_cols']\n",
        "num_cols = dictt['num_cols']\n",
        "print(dictt['ordered_cols'])"
      ],
      "metadata": {
        "id": "etM-FRc_hHaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we have a large amount of variables, so we need to handle them manually in preprocessing. \n",
        "#this csv file keeps variable names, their real dtypes and decide whether they should be discarded or not \n",
        "var_df = pd.read_csv('varlist_fbb_new.csv', na_values = '', delimiter = ', ') \n",
        "var_df['name'] = var_df ['name'].str.lower()\n",
        "\n",
        "var_df.loc[:, 'discard'] = 1\n",
        "var_df.loc[var_df['name'].isin(['dsl_no']), 'discard'] = 0\n",
        "\n",
        "var_df.loc[var_df['name'].isin(dictt['ordered_cols']), 'discard'] = 0\n",
        "\n",
        "var_drop = list(var_df['name'][var_df['discard']==1])\n",
        "\n",
        "var_df.drop(var_df.loc[var_df['name'].isin (var_drop) ==True].index, inplace=True)\n",
        "var_df.reset_index(drop=True, inplace=True)\n",
        "print(var_df.head())\n",
        "\n",
        "str_vars=\", \".join(var_df['name'])\n",
        "print(str_vars)"
      ],
      "metadata": {
        "id": "_B2oLbuckR9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pre_df['dsl_no'] = pre_df['dsl_no'].apply(lambda x: + str(x) + \"*\")\n",
        "#dsl_vars=\", \".join(pre_df['dsl_no'])\n",
        "#print (pre_df.head())\n",
        "\n",
        "if script_pars['skipdbread'] != 1:\n",
        "  #your query\n",
        "  query = \"\"\"select \"\"\" + str_vars + \"\"\" from \"\"\" + script_pars['table_to_use'] + \"\"\" \"\"\" + script_vars['sme_limit']+ \"\"\" ;\"\"\" \n",
        "  #print(query)\n",
        "  conn = create_gp_engine()\n",
        "  dfs= []\n",
        "  chunksize=script_pars['chunksize']\n",
        "  i=0\n",
        "  for chunk in pd.read_sql_query(query, con con = conn, chunksize=chunksize):\n",
        "    print(i)\n",
        "    dfs.append(chunk)\n",
        "    i=i+1\n",
        "    pre_df= pd.concat(dfs)\n",
        "else:\n",
        "  pass"
      ],
      "metadata": {
        "id": "kwhEoRBVkSAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "model_dir = os.getcwd() + \"/model_results/\" + script_pars['preferred_folder_name'] + \"/\"\n",
        "\n",
        "voting_dict = {}\n",
        "\n",
        "with open(model_dir+\"/voting_results.txt\", 'r') as f:\n",
        "  voting_str = f.read()\n",
        "\n",
        "with open(model_dir+\"/voting_results.pkl\", 'rb') as f: \n",
        "  voting_dict = pickle.load(f)\n",
        "\n",
        "with open(model_dir+\"/voting_df.pkl\", 'rb') as f: \n",
        "  voting_df = pickle.load(f)\n",
        "\n",
        "#model_success_df = pd.read_csv(model_dir + 'voting_models.csv', na_values = '', delimiter = ')\n",
        "print(voting_df)"
      ],
      "metadata": {
        "id": "CsHIL9N7kSC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(model_dir+\"/bins.pkl\", 'rb') as f: \n",
        "  bins_dict = pickle.load(f)\n",
        "print(bins_dict)"
      ],
      "metadata": {
        "id": "q9t-XRFMkSFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_n_models = script_pars['top_n_models']\n",
        "\n",
        "voter_models = voting_df['model'].iloc[:top_n_models].unique()\n",
        "voter_models.sort()\n",
        "print(voter_models)"
      ],
      "metadata": {
        "id": "yONlH6CskSIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from glob import glob\n",
        "preferred_folder_name = script_pars['preferred_folder_name']\n",
        "\n",
        "if preferred_folder_name == '';\n",
        "  #model_dirs = [x[0] for x in os.walk (os.getcwd())]\n",
        "  model_dirs = glob(os.getcwd()+\"/model_results/*/\", recursive = True)\n",
        "  model_dirs_filtered = [x for x in model_dirs if script_pars['run_type'] in x] \n",
        "  model_dirs_filtered.sort(reverse = True)\n",
        "  model_dir = model_dirs_filtered[0]\n",
        "\n",
        "  model_list = [s for s in os.listdir(model_dir) if any (x in s for x in voter_models)]\n",
        "\n",
        "elif preferred_folder_name != '':\n",
        "  model_dir = os.getcwd() + \"/model_results/\" + preferred_folder_name + \"/\"\n",
        "  model_list = [s for s in os.listdir (model_dir) if any (x in s for x in voter_models)]"
      ],
      "metadata": {
        "id": "MT1d1hHrmHs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = []\n",
        "for i in range(len(model_list)):\n",
        "  with open(model_dir+\"/\"+model_list[i], 'rb') as f:\n",
        "    model = pickle.load(f)\n",
        "    models.append(model)"
      ],
      "metadata": {
        "id": "vZZavk_bmHwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if script_pars['skipdbread'] != 1:\n",
        "  import math\n",
        "  print(pre_df.head())\n",
        "  print(len(pre_df))\n",
        "  batch_nb = math.ceil(len(pre_df)/1000000) \n",
        "  print(batch_nb)\n",
        "\n",
        "  for i in range(batch_nb):\n",
        "    print(i)\n",
        "    sub_df = pre_df.iloc[i*1000000: (i+1)*1000000]\n",
        "    sub_df.to_csv(model_dir+\"/batch_\"+str(i)+\".csv\", index = False)\n",
        "    print(len(sub_df))\n",
        "else:\n",
        "  batch_nb = script_pars['batch_nb']"
      ],
      "metadata": {
        "id": "riU3ld8-mHyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'skipdbread': 0, #If you already read DB succesfully and wrote into model_folder as csv batches, you can skip this part and write as csv part by making it 1 \n",
        "'skippropwrite': 0, #If you already wrote propensity outputs into model_folder as csv file batches, you can skip this part too\n",
        "                    #0: write everything, owerwrite can occur\n",
        "                    #1: If your kernel died during run, and you are sure input set didnt change, you can set it 1 to prevent overwrite, it wont save if name already there \n",
        "                    #2: skip everything, do not write any result, go on with what is already there.\n",
        "'rmallpropcsv': 0, #if 1, it removes every result csv in directory. Be careful while using it\n",
        "\n",
        "#If you set to remove all result csvs in the beginnning id does that\n",
        "if script_pars['rmallpropcsv'] == 1:\n",
        "  filelist = [f for f in os.listdir (model_dir) if f.endswith(\".csv\")] \n",
        "  proplist = [f for f in filelist if f.startswith(\"propensity_\")] \n",
        "  for f in proplist:\n",
        "    os.remove(os.path.join(model_dir, f))\n",
        "\n",
        "if (script_pars['skippropwrite'] == 0) or (script_pars['rmallpropcsv'] == 1): \n",
        "  batcher = range(batch_nb)\n",
        "  filelist = [f for f in os.listdir(model_dir) if f.endswith(\".csv\")] \n",
        "  proplist = [f for f in filelist if f.startswith(\"propensity_\")]\n",
        "\n",
        "elif script_pars['skippropwrite'] == 1:\n",
        "  ###try to get numbers from propensity names as a list and ignore them"
      ],
      "metadata": {
        "id": "rEU3Br7JmH1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in range(batch_nb):\n",
        "  print(batch)\n",
        "  sub_df = pd.read_csv(model_dir+\"/batch_\"+str(batch)+\".csv\") \n",
        "  print(len(sub_df))\n",
        "  print(sub_df.head())\n",
        "  sub_df.columns = sub_df.columns.str.lower()\n",
        "  var_drop = [col.lower() for col in var_drop]\n",
        "  sub_df.drop(columns = [col for col in var_drop if col in sub_df.columns), axis= 1, inplace = True)\n",
        "\n",
        "  sub_df = sub_df.replace('NA', np.nan)\n",
        "  sub_df = sub_df.replace('NULL', np.nan)\n",
        "\n",
        "  #var_df['type'] = np. where(war_df['name'].isin([xxxx]), 'category', var_df['type']) \n",
        "  var_df['type'] = np.where(var_df['type'].isin(['int']), 'Int64', var_df['type']) \n",
        "  print(sub_df.isna().sum())\n",
        "\n",
        "  pre_df_cols = sub_df.columns\n",
        "  varlist = list(var_df ['name'].str.lower())\n",
        "  print(len(pre_df_cols))\n",
        "  \n",
        "  var_df.loc[var_df['name'].isin(['feata', 'featb']), 'type'] = 'category'\n",
        "  sub_df['featc'] = sub_df['featc'].replace('4 GB', '4096') \n",
        "  sub_df['featc'] = sub_df['featc'].replace('6 GB', '6144') \n",
        "  sub_df['featc'] = sub_df['featc'].replace('Limitsiz', '204800')\n",
        "\n",
        "  #changing python dtypes and if they are different than our list, change dtype. Float thing was not necessary, but after so many errors inside pipe, i needed to do it \n",
        "  for c in range(len(pre_df_cols)):\n",
        "    print(c)\n",
        "    if var_df.loc[c, 'type'] == 'datetime64':\n",
        "      #main_df[main_df_cols[i]] = pd.to_datetime (main_df[main_df_cols[i]], format = var_df.loc[i, 'timetype'], errors = 'coerce')\n",
        "      pass\n",
        "    if var_df.loc[c, 'type'] == 'Int64':\n",
        "      print(pre_df_cols[c])\n",
        "      print(var_df.loc[c, 'type'])\n",
        "      sub_df[pre_df_cols[c]] = sub_df[pre_df_cols[c]].astype('float')\n",
        "      sub_df[pre_df_cols[c]] = sub_df[pre_df_cols[c]].astype(var_df.loc[c, 'type'])\n",
        "    else:\n",
        "      print(pre_df_cols[c])\n",
        "      print(var_df.loc[c, 'type'])\n",
        "      sub_df[pre_df_cols[c]] = sub_df[pre_df_cols[c]].astype(var_df.loc[c, 'type'])\n",
        "\n",
        "  if [m for m in ['feata', 'featb'] if m in sub_df.columns]: \n",
        "    print(\"It is in colummns\")\n",
        "    ### !!!!!!! This is an important concept with 2 outcomes to be used in future. This cell is only for test purpose. \n",
        "    ### Replace is not workint with INPLACE parameter anymore, you need to specify it explicitly\n",
        "    ### And if the DTYPE is not OBJECT, replacing NP. NAN value with a fixed string value is not working. You need to change DTYPE pre_df_test = sub_df.copy()\n",
        "    pre_df_test[['feata', 'featb']] = pre_df_test[['feata', 'featb']].replace(np.nan, 'NA') \n",
        "    #print(pre_df_test[['feata', 'featb']].head (10))\n",
        "\n",
        "    pre_df_test[['feata', 'featb']] = pre_df_test[['feata', 'featb']].astype('object') \n",
        "    pre_df_test[['feata','featb']] = pre_df_test[['feata', 'featb']].replace(np.nan, 'NA')\n",
        "    #print(pre_df_test[['feata\", 'featb']].head(10))\n",
        "\n",
        "    ### Here is the 2nd variatoin, using fillna. But it needs to see it as a new category\n",
        "    ### And also add_category is only using series, one column each time. So, i used a lambda funtion to apply it properly\n",
        "    #These features are filled with NA because features are populated only if there is a cancellation application. So, missing values have meaning here\n",
        "\n",
        "    sub_df[['feata', 'featb']] = sub_df [['feata', 'featb']].apply(lambda x: x.cat.add_categories('NA').fillna('NA')) \n",
        "    #print(value[['feata','featb']].head())\n",
        "\n",
        "  ### Decided to fill all categorical NaN values with NA category.\n",
        "  #print(value.dtypes.astype (str).value_counts())\n",
        "  date_cols = list(sub_df.select_dtypes(include = ['datetime64']).columns)\n",
        "  \n",
        "  #num_cols = list(pre_df.select_dtypes (include = ['float', 'int', 'int64', 'Float64', 'Int64', 'float64']).columns)\n",
        "  num_cols = dictt['num_cols']\n",
        "  #cat_cols = list(pre_df.select_dtypes (exclude = ['float', 'int', 'int64', 'Float64', 'Int64', 'float64', 'datetime64']).columns) \n",
        "  cat_cols = dictt['cat_cols']\n",
        "  \n",
        "  sub_df.drop(columns = date_cols, axis= 1, inplace=True)\n",
        "\n",
        "  pre_df1 = sub_df.iloc[:500000,]\n",
        "  pre_df2 = sub_df.iloc[500000:.]\n",
        "\n",
        "  #!! category dtype do not let you replace or fill np.nan's with new category value, that is why i switched to str type \n",
        "  #divided df into 2 as it was having memory error\n",
        "  for col in cat_cols:\n",
        "    pre_dfl[col] = pre_dfl[col].astype('object')\n",
        "    pre_dfl[col] = pre_dfl[col].replace (np.nan, 'NA')\n",
        "\n",
        "  for col in cat_cols:\n",
        "    pre_df2[col] = pre_df2 [col].astype('object')\n",
        "    pre_df2[col] = pre_df2[col].replace(np.nan, 'NA')\n",
        "\n",
        "  sub_df = pre_df1.append(pre_df2, ignore_index = True)\n",
        "\n",
        "  #print(value.dtypes.astype(str).value_counts())\n",
        "\n",
        "  if [i for i in ['feat_a'] if i in sub_df.columns]:\n",
        "    sub_df['feat_a'] = sub_df['feat_a'].astype('str').replace(r'^\\+', '', regex=True)\n",
        "    sub_df['feat_a'] = sub_df['feat_a'].astype('object')\n",
        "\n",
        "  main_df = sub_df.reindex(columns= dictt['ordered_cols'])\n",
        "\n",
        "  # read_csv broke the behavior of df, changedd numerical categorical values, added .0 to their end\n",
        "  # It is for correcting them \n",
        "  for col in cat_cols:\n",
        "    main_df[col] = main_df[col].astype('str').replace(r'\\.0$', '', regex=True)\n",
        "\n",
        "  main_dsl_df = sub_df.loc[:, 'dsl_no']\n",
        "\n",
        "  inference = pd.DataFrame()\n",
        "  for md in range(len(models)):\n",
        "    inference['proba' + str(md)] = models[md].predict_proba(main_df)[:,1]\n",
        "    inference['predict' + str(md)] = models[md].predict(main_df)\n",
        "  \n",
        "  inference['avg_proba'] = inference[[col for col in inference.columns if 'proba' in col]].mean(axis=1)\n",
        "  inference['voted_is_sale'] = np.where(inference[[col for col in inference.columns if 'predict' in col]].sum(axis=1)/len(models) <0.5, 0,1) \n",
        "  #inference inference.join(df_y_final['is_sale'])\n",
        "  #print (inference)\n",
        "\n",
        "  dsl_df = pd.DataFrame(main_dsl_df, columns = ['dsl_no'])\n",
        "  inference['dsl_no'] = dsl_df.loc[:, 'dsl_no']\n",
        "  groups_list = [10,20,40,80]\n",
        "\n",
        "  for ii in range(len(groups_list)):\n",
        "    inference[\"PropensityGroup_bin \"+str(groups_list[ii])] = pd.cut(inference[\"avg_proba\"], bins=np.array(bins_dict[groups_list[ii]]), labels=False, include_lowest=True) \n",
        "  \n",
        "  publish_df = pd.DataFrame()\n",
        "  publish_df = inference.loc[:, ['dsl_no', 'avg_proba', 'PropensityGroup_bin10', 'PropensityGroup_bin20', 'PropensityGroup_bin40', 'PropensityGroup_bin80']] \n",
        "  print(publish_df.head(5))\n",
        "  print(len(publish_df))\n",
        "\n",
        "  publish_df.to_csv(model_dir+\"/propensity_\"+str(batch)+\".csv\", index = False)\n"
      ],
      "metadata": {
        "id": "MOIrDjFqmH4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### If val_test.csv added into training pipeline, copy paste above cell. Delete the for loop. Fetch the csv file and run this cell to analyze it. Check consistenct with training pipeline\n",
        "\n",
        "sub_df = pd.read_csv(model_dir+\"/val_test.csv\")\n",
        "###\n",
        "###\n",
        "###\n",
        "###"
      ],
      "metadata": {
        "id": "NbNn2lS1zWlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "publish_df_all = pd.DataFrame()\n",
        "\n",
        "for batch in range(batch_nb):\n",
        "  print(batch)\n",
        "  sub_df = pd.read_csv(model_dir+\"/propensity_\"+str(batch)+\".csv\")\n",
        "  publish_df_all = sub_df.append(publish_df_all, ignore_index = True) \n",
        "  print(len(publish_df_all))"
      ],
      "metadata": {
        "id": "s2PfuSTBsCYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import sqlalchemy as sa\n",
        "#from sqlalchemy import create_engine\n",
        "#from sqlalchemy.engine import url as sa_url\n",
        "\n",
        "#def create_mysql_engine(host, database, username, password, port='3307'):\n",
        "  #url = sa_url.URL(drivername=\"mysql+pymysql', host=host, port=port, database=database,\n",
        "  #username=username, password=password)\n",
        "  #print(url)\n",
        "  #return sa.create_engine(url)\n",
        "\n",
        "#engine=create_mysql_engine (ai['host'], 'smexplorerdata_fbb', ai['user'], ai['password'], ai['port'])\n",
        "#publish_df_all.to_sql(if_exists = 'replace', name= 'fbb_retention_propensity_groups_crm_test', con = engine, schema= 'smexplorerdata_fbb', index = False)"
      ],
      "metadata": {
        "id": "6GpNUwx0sCcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#engine=create_mysql_engine (ai['host'], 'smexplorerdata_fbb', ai['user'], ai['password'], ai['port'])\n",
        "#with engine.connect() as con:\n",
        "#con.execute('CREATE INDEX fbb_retention_propensity_groups_crm_test_dsl_no_IDX USING BTREE ON smexplorerdata_fbb.fbb_retention_propensity_groups_crm_test (dsl_no (100));')"
      ],
      "metadata": {
        "id": "GaOFfpQxsCfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlalchemy as sa\n",
        "from sqlalchemy import create engine\n",
        "from sqlalchemy.engine import url as sa_url\n",
        "print(publish_df_all)\n",
        "conn = create_gp_engine()\n",
        "publish_df_all.to_sql(\"propensity_feed_v2\", con-conn, chunksize=10000, method= \"multi\", schema= \"afiniti_aida_fbb\", if_exists= \"replace\", index= False)"
      ],
      "metadata": {
        "id": "4ycnG1CksCij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "engine=create_gp_engine()\n",
        "with engine.connect() as con:\n",
        "  con.execute('CREATE INDEX propensity_feed_v2_dsl_no_idx ON afiniti_aida_fbb.propensity_feed_v2 (dsl_no);')"
      ],
      "metadata": {
        "id": "u0dUz-aOt7bo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}